# ğŸ§  AI Modules for FastAPI Project

This module provides core AI functionalities to be integrated into a FastAPI backend. It includes LLM loading, PDF processing, and answer generation using a RAG (Retrieval-Augmented Generation) pipeline.

---

## âš™ï¸ Model Configuration

| Component     | Description                                  |
|---------------|----------------------------------------------|
| **LLM Model** | Vicuna 7B - lmsys/vicuna-7b-v1.5   |
| **Embeddings**| bkai-foundation-models/vietnamese-bi-encoder |

---

## ğŸ“¦ AI Functions Available

There are **3 main functions** that the backend (BE) team can use:

1. `load_llm()`  
   â¤ Loads and returns the LLM pipeline (with quantization and tokenizer setup).

2. `process_pdf(llm, pdf_path, embeddings)`  
   â¤ Accepts a PDF file path, performs semantic chunking, creates a retriever for question answering.

3. `generate_answer(rag_chain, question)`  
   â¤ Uses a LangChain `rag_chain` pipeline to answer a question based on retrieved context.

---

## ğŸ§ª Testing the AI Functions

### 1ï¸. Run the **standalone AI tests**

This will test AI logic independently:

```bash
python -m ai_modules.test_ai_modules
```
### 2. Run the **FastAPI-integrated AI tests**

This will test AI functions with FastAPI:

```bash
python -m ai_modules.test_ai_modules_with_fastAPI
```

## âš™ï¸ Folder Structure (Simplified)
```
project/
â”œâ”€â”€ ai_modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ load_llm.py                      # Contains load_llm()
â”‚   â”œâ”€â”€ pdf_processor.py                 # Contains process_pdf()
â”‚   â”œâ”€â”€ generate.py                      # Contains generate_answer()
â”‚   â”œâ”€â”€ test_ai_modules.py               # standalone AI function test
â”‚   â””â”€â”€ test_ai_modules_with_fastAPI.py  # FastAPI-integrated AI tests
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py                 
```
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py                 
```
